---
# Copyright 2017 Yahoo Holdings. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Semantic Search using Text Embedding Tensors "
---

<p>
This document describes how to represent <a href="https://en.wikipedia.org/wiki/Word_embedding">text embedding</a> tensors in Vespa and 
how to build a real time semantic search engine using  Google's <a href="https://arxiv.org/abs/1803.11175">Universal Sentence Encoder</a> which 
transforms a variable length text sentence to a fixed-length dense tensor. 
We demonstrate how to build a semantic search engine which given a query shows semantically relevant passages from web pages using a sample dev dataset from MS MARCO.
</p>

<p>
Vespa has strong support for storing tensors and performing operations over tensors fields in addition to traditional field types and ranking features 
like <a href="reference/bm25.html">bm25</a> or Vespa's <a href="reference/nativerank.html">nativeRank</a>.  Having both traditional text ranking features and semantic similarity features
expressed in the same engine is a powerful feature of Vespa. Tensor fields can be updated in user time at scale like any other field in Vespa enabling incremental updates.
</p>


<h2 id="universal-sentence-encoder">About Google's Universal Sentence Encoder</h2> 
<p>
The Universal Sentence Encoder encodes text into high-dimensional tensors that can be used for broad range of downstream tasks such as text classification, semantic similarity, semantic retrieval,
clustering and other natural language processing (NLP) tasks. Google has released several different sentence encoder models with different goals and in our experiments we use
a model trained on English, even there exist multilingual encoders. 

</p>

<figure>
<img src="https://www.gstatic.com/aihub/tfhub/universal-sentence-encoder/example-similarity.png" alt="Image Courtesy https://tfhub.dev/google/universal-sentence-encoder/2" width="1272" height="280"/>
<figcaption>Image Courtesy <a href="https://tfhub.dev/google/universal-sentence-encoder/2">https://tfhub.dev/google/universal-sentence-encoder/2</a></figcaption>
</figure>
</p>
<p>

<p>Papers and resources on Google's Universal Sentence Encoder:
<ul>
<li><a href="https://arxiv.org/abs/1803.11175">Universal Sentence Encoder Paper</a></li> 
<li><a href="https://arxiv.org/abs/1907.04307">Multilingual Universal Sentence Encoder for Semantic Retrieval Paper</a> </li>
<li><a href="https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html">Google AI Blog: Multilingual Universal Sentence Encoder for Semantic Retrieval</a> </li>
<li><a href="https://tfhub.dev/google/universal-sentence-encoder/2">Tensorflow Hub</a> </li>
<li><a href="https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb">Semantic Similarity with TF-Hub 
Universal Encoder (Google Colab Jupyter notebook)</a> </li>
</ul>

In this document we use the <a href="https://tfhub.dev/google/universal-sentence-encoder/2">https://tfhub.dev/google/universal-sentence-encoder/2</a> model which is about 1GB and
is trained with a deep averaging network (DAN) encoder. In the below example snippet we find the tensor representation of 3 sentences and calculate the semantic similarity between them
by calculating the <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a>

<pre>
import tensorflow as tf
import tensorflow_hub as hub
import numpy as np

module_url = "https://tfhub.dev/google/universal-sentence-encoder/2"
embed = hub.Module(module_url)
text_sentences = tf.compat.v1.placeholder(tf.string)
embeddings = embed(text_sentences)

session = tf.compat.v1.Session() 
session.run([tf.compat.v1.global_variables_initializer(), tf.compat.v1.tables_initializer()])

sentences = [
    "How old are you?",
    "What is your age?",
    "Where do you live?"
  ]
tensors = session.run(embeddings,feed_dict={text_sentences:sentences})
#Calculate semantic similarity using cosine similarity:
np.dot(tensors[0],tensors[1])
> 0.8516872
np.dot(tensors[0],tensors[2])
>0.3982244
np.dot(tensors[0],tensors[0])
>1.0
</pre>

<h2 id="dataset">About the MS MARCO dataset</h2> 
<p>The <a href="http://www.msmarco.org/">MS MARCO</a> dataset has several tracks and tasks accociated with it. We use the simple <em>Question Answering V1.1</em> data set which is 
published in an easy to consume JSON format. We don't apply any <a href="learning-to-rank.html">learning to rank</a> techniques so we can use
the dev set directly without any train/test split.</p>
<p> 
This data set contains 10047 queries where 9706 queries have at least one passage which is marked as relevant
Queries are categorized into 5 unique query types. The query type distribution for the set of 9706 queries which has at least one relevant passage is given below:  

<ul>
<li>5243 description</li>
<li>2713 numeric</li>
<li>1010 entity</li>
<li>502 location</li>
<li>238 person</li>
</ul>

The 9706 queries which have at least one relevant passage has a total of 79,647 passages so the average passages per query is 8.2. 
</p>

<p>A sample from the dataset is shown below (With some text passages emitted and some of the text truncated for readability). 
The data also contains a curated textual answer for some of the queries but we 
ignore this in our evaluation. The 'is_selected' takes the value 1 if the passage is marked relevant. The editorial judgement is consequently binary relevant or not relevant. </p>
</p>
<pre>
{
    "query": "walgreens store sales average",
    "query_id": 9652,
    "query_type": "numeric"
    "passages": [
        {
            "is_selected": 1,
            "passage_text": "The average Walgreens salary ranges from approximately $15,000 per year for Customer Service...",
            "url": "http://www.indeed.com/cmp/Walgreens/salaries"
        },
        {
            "is_selected": 0,
            "passage_text": "Your Walgreens Store. Select a store from the search results to make it Your Walgreens Store and save time getting what you need.... ",
            "url": "http://www.walgreens.com/storelocator/find.jsp?requestType=locator"
        }
    ]
}
</pre>

<h2 id="creating-the-feed">Creating the Vespa feed</h2>
<p>In order to feed the data set to our Vespa instance we need to convert it to the <a href="reference/document-json-format.html">Vespa document json format</a>. 
Since the Universal Sentence Encoder is trained on sentence length text we use 
a simple <a href="https://www.nltk.org/api/nltk.tokenize.html">sentence tokenizer</a> to tokenize the passage text into sentences. 
The sentence tokenization could be done inside the Vespa cluster as well using the 
<a href="document-processing.html">document processing</a> framework with custom <a href="linguistics.html">Linguistic</a> processing. </p>

<p<We also include the number of sentences extracted from the passage along 
with the relevance label (is_selected) and the query_id. Each document sample from the dev set is split into one Vespa document per passage and we assign an unique document 
id per passage so in cases where 
a passage is listed for multiple queries it would be a separate unique document. 
Below is a sample of the Vespa json document representation of the relevant text passage for query 9652:</p>

<pre>
{
  "put": "id:msmarco:passage::0"
  "fields": {
    "is_selected": 1,
    "n_sentences": 3,
    "passage_text": "The average Walgreens salary ranges from approximately $15,000 per year for Customer Service.... ",
    "query": "walgreens store sales average",
    "query_id": 9652,
    "query_type": "numeric",
    "sentences": [
      "The average Walgreens salary ranges from approximately $15,000 per .. for Customer .. Associate .. $179,900 ... for District Manager.",
      "Average Walgreens hourly pay .. $7.35 per hour for .. for Pharmacy Manager.",
      "Salary information comes from ... , users, and jobs on Indeed."
     ]
  }
}
</pre>


<h2 id="model" >Vespa document model with tensors</h2>
<p>Vespa supports a <a href="reference/search-definitions-reference.html#type:tensor">tensor field type</a> which allows 
representing high dimensional tensors of any order. 
The Vespa tensor field type cannot be searched as regular <a href="reference/search-definitions-reference.html#field-types>field types</a> but could be 
used for scoring and ranking.</p>

<p>
We use the Sentence Encoder to encode 
each of the tokenized sentences from the passage text into their tensor representation. The sentence embedding tensors are stored in a second order tensor (matrix)
where there is a map between the sentence id and the corresponding dense sentence tensor representation. 
We also use the Sentence Encoder to encode the entire passage text into its tensors representation which is 
stored in a dense first order tensor (vector) with 512 dimensions.   

In the below example <a href="search-definitions.html">document definition</a> we define our <em>passage</em> document type :

<pre>
search passage {
  document passage {

    field sentences type array&lt;string&gt; {
      indexing: summary | index
      index: enable-bm25
    }

    field passage_text type string {
      indexing: summary | index
      index: enable-bm25
    }

    field query_id type int { 
      indexing: summary |attribute
    }

    #First order tensor. Dense tensor[] with dimensionality 512
    field passage_embedding type tensor&lt;float&gt;(x[512]) {
      indexing: attribute
    }
    #Second order mixed tensor map sparse {} sentence id to the dense encoded sentence tensor 
    field sentence_embeddings type tensor&lt;float&gt;(s{},x[512]) {
      indexing: attribute
    }
 }
}
</pre>
<p>We define the <em>sentences</em> as an array of string which we build a index for and also return in the summary (SERP) and similar the <em>passage_text</em> is
defined as a single valued field and indexed.  

<p>Using the sentence encoder over the passage text and each of the extracted sentences our example Vespa feed document becomes:</p>
<pre>
{
  "put": "id:msmarco:passage::0"
  "fields": {
    "passage_text": "The average Walgreens salary ranges from approximately $15,000 per year for Customer Service.... ",
    "sentences": [..],
    sentence_embeddings": {
      "blocks": {
        "0":[-0.02239381894469261,0.053109243512153625, ..],
        "1":[-0.0376732237637043,  0.05585171654820442,..] 
        "3":[-0.02057761326432228}, 0.08141108602285385,..]
      },
    "passage_embedding": {
      "values": [-0.04661116376519203, 0.05944991856813431,..] 
    }
  }
}
</pre>
<p>Some fields are left out for readability and not all dimension values are included.</p> 

<h2 id="ranking">Ranking passages</h2>
<p>We have our feed with  79,647 documents indexed in a Vespa instance configured with the above schema and we can start exploring how to query and rank the passages but first
we need to look at how to calculate the textual semantic similarity.</p> 

<h3 id="semantic-similarity">Semantic Similarity and using Tensor Operations</h3>
<p>The tensor produced by the Google Universal Sentence Encoder is according to the authors approximately normalized so we can use
the inner dot product between the query and document tensors as our textual semantic similarity function. The inner dot product will compute the same
score as the cosine similarity when tensors are normalized. 

The scoring formula involving tensors uses <a href="reference/tensor.html#operations">Tensor operations</a> which can be combined with <a href="reference/rank-features.html">Vespa rank features</a>
expressed by Vespa <a href="reference/ranking-expressions.html">ranking expression</a>. Examples of tensor operations where A, B are dense or sparse 1-order tensor (vectors): </p> 

<pre> 
sum(A * B) #inner dot product
sum(A * B) / ( sqrt(sum(A*A)) + sqrt(sum(B*B)) ) # Cosine similarity 
sqrt(sum(map(A - B, f(x)(x * x)))) # Euclidian distance
reduce(sum(Q*D,x),max) #Vector x Matrix 
</pre>
<p>More examples tensors and tensor operations in <a href="tensor-intro.html">tensor intro</a>

<h3 id="recall-strategies">Query &amp; Recall strategies</h3>
<p> 


</p>

<h3 id="ranking-models">Ranking Models</h3>
<p>In this section 

<b>Passage ranking model 1: Passage level scoring </b>
<p>In this model we score passages by their inner dot product between the query tensor (produced by the Sentence Encoder and where the query is represented used as the sentence) and the document tensor.</p>
<pre>
rank-profile passage-scoring inherits default {
  first-phase {
    expression: sum(query(tensor)*attribute(passage_embedding))
  }
}
</pre>

<b>Passage ranking model 2: Sentence level scoring </b>
<p>In this model we compute the max of the inner dot product between the query tensor and the set of sentence tensors extracted from the original text passage.</p> 
<pre>
rank-profile max-sentence-scoring inherits default {
  first-phase {
    expression: reduce(sum(query(tensor)*attribute(sentence_embeddings),x),max) 
  }
}
</pre>

<b>Passage ranking model 3: random </b>
<p>Vespa has a built-int random ranking feature which we can use as a baseline when evaluating the effectiveness of our approaches.</p>

<pre>
rank-profile random inherits default {
  first-phase {
    expression: random 
  }
}
</pre>

<b>Passage ranking model 4: bm25 </b>
<p>Using standard BM25 model to rank passages. Parameters TODO.
<pre>
rank-profile bm25 inherits default {
  first-phase {
    expression: bm25(passage_text) 
  }
}
</pre>

<b>Passage ranking model 5: nativeRank </b>
<pre>
rank-profile nativeRank inherits default {
  first-phase {
    expression: nativeRank(passage_text) 
  }
}
</pre>

<b>Passage ranking model 6: Linear combination of nativeRank and semantic similarity from model 2 </b>
<p>This model uses a linear combination of the tensor similarity and Vespa's nativeRank.</p>
<pre>
rank-profile nativeRank-and-max-sentence-linear  inherits default {
  first-phase {
    expression: 0.5*nativeRank(passage_text) + 0.5*reduce(sum(query(tensor)*attribute(sentence_embeddings),x),max)
  }
}
</pre>

<h2 id="model-evaluation">Evaluating the models</h2>
<p>Given that we have labeled data with relevant/not-relevant we can run the dev test queries and evaluate the effectiveness of our suggested ranking profiles. We use the <a href="https://en.wikipedia.org/wiki/Mean_reciprocal_rank">mean reciprocal rank</a>
to evaluate the results of ranking model. The MRR@10 is also the metric used in most of the various MS MARCO tasks. Since there is on average close to 1 relevant passages per query we believe this metric is a good fit for evaluating our models..</p>

<p>In the first experiment we run through all 9706 queries and for each query we recall only passages which are asscociated with the query id so at most we recall 10 documents per query so the comparision in the table is only about the ordering (ranking) 
of the passages.</p>

<h3 id="Model-comparision">Model Ranking metrics</h3>
<table style="width:80%">
  <tr>
    <th><b>Model</b></th>
    <th>MRR@10 all (9706 queries)</th>
    <th>MRR@10 category 'description' (5243 queries)</th>
    <th>MRR@10 category 'entity' (1010 queries)</th>
  </tr>
  <tr>
    <td>Model 1 Passage semantic scoring </td>
    <td>0.4384211696904876</td>
    <td>0.43569324481016264</td>
    <td>0.40996306773534497</td>
  </tr>
  <tr>
    <td>Model 2 Sentence level semantic scoring</td>
    <td>0.4520007669940701</td>
    <td>0.44573285923181016</td>
    <td>0.4465535124941066</td>
  </tr>

  <tr>
    <td>Model 3 random scoring </td>
    <td>0.35343789964643046</td>
    <td>0.3558203833380259</td>
    <td>0.3486507936507937</td>
  </tr>
  <tr>
    <td>Model 4 bm25 scoring </td>
    <td>0.4715941947216416</td>
    <td>0.4746613776796878</td>
    <td>0.4500911519723401</td>
  </tr>

  <tr>
    <td>Model 5 nativeRank scoring</td>
    <td>0.4998918603776423</td>
    <td>0.5100987257386266</td>
    <td>0.48910694640892666</td>
  </tr>

  <tr>
    <td>Model 6 nativeRank + Sentence level semantic scoring</td>
    <td>0.49637681159420294</td>
    <td>0.4995049332594631</td>
    <td>0.48609303787521607</td>
  </tr>
</table>
<p>Result discussion</p>

<p>In our second experiment we don't limit the recall to only passages which are associated with the query id but rank all passages and calculate the MRR@10.  

<h2 id="scaling">Serving Performance & Scalability</h2>
<p>The tensor evaluation <a href="performance/sizing-search.html">scales</a> like any other ranking feature but the cost is quite high if the recalled set of documents is high. The above examples uses a query which 
matches all documents of document type passage, hence the performance scales with the number of documents. To bring down latency we can use more threads per query and partition the data over more nodes.
</p>

<h2 id="phased-execution>Phased Ranking</h2>
<p>



</p>

