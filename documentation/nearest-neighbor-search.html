---
# Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root. 
title: "Nearest Neighbor Search"
---
<p>
Searching for the nearest neighbors of a data point in high dimensional vector space is an important problem for many real time applications. 
For example, in Computer Vision, searching for close data points in high dimensional vector space enables finding the most similar cats in large image datasets. 
In Information Retrieval, large pre-trained natural language Transformer models like BERT, allows representing text sentences in dense embedding space, 
where nearest neighbor search could serve as an effective multilingual neural retrieval function.
</p><p>
In many of these real word applications of (approximate) nearest neighbor search, the search is constrained by real time query filters 
applied over the data points metadata. For example, in E-Commerce search applications with constantly evolving metadata, 
a search for nearest products for a query in vector space would typically be constrained by inventory status and price. 
</p><p>
Expressing the search for nearest neighbors as a query operator allows combining the search for nearest neighbors with traditional query filters using boolean query operators.

Since tensor are just regular fields in Vespa one can define multiple tensor fields
in the same document schema.
</p>

<h2 id="using-vespas-nearest-neighbor-search">Using Vespa's nearest neighbor search</h2>
<p>The following sections demonstrates how to use the nearest neighbor search operator with Vespa, the examples uses the brute force variant which has
perfect accuracy but which is computationally expensive for large document volumes as the distance needs to be calculated for every document which matches 
the boolean query filters in the query. See how to speed up evaluation of nearest neighbor search by adding a <em>HNSW</em> index in
the <a href="approximate-nn-hnsw.html">Approximate Nearest Neighbor Search</a>. 

<p>We demonstrate how to use the nearestNeighbor query operator using a <a href="use-case-shopping.html">E-Commerce search use cases</a>: 

<ul>
  <li>Related products by product image similarity using SIFT image encodings</li>
  <li>Related products using text (title) embeddings similarity from a Transformer model (BERT)</li>
  <li>Related products using a hybrid combination of the above</li>
  <li>Hybrid Search using traditional IR term based matching in combination with neural vector space retrieval</li>
</ul> 

We define the following product <a href="schemas.html">document schema</a>:
</p>

<pre>
search product {

  document product {
   
    field title type string {
      indexing: summary | index
    }

    field in_stock type bool {
      indexing: summary | attribute 
    }

    field price type float {
      indexing: summary | attribute 
    }

    field popularity type float {
      indexing: summary | attribute 
    }

    field image_sift_encoding type tensor<float>(x[128]) {
      indexing: summary | attribute
      attribute {
        distance-metric: euclidean
      }
    }

    field title_bert_embedding type tensor<float>(x[768]) {
      indexing: summary | attribute
      attribute {
        distance-metric: angular
      }
    }
  }
}
</pre>
<p>We defined a <em>product</em> document type which is made up of six fields. Fields like title, in_stock, price and popularity are self-explanatory but the 
<a href="tensor-user-guide.html">tensor</a> type fields needs more explanation. 
 
<p>The <em>image_sift_encoding</em> is used to store the 
<a href="https://en.wikipedia.org/wiki/Scale-invariant_feature_transform">scale-invariant feature transform (SIFT)</a> encoding of an product catalog image.
The tensor definition specifies that it's dense [] and has exactly 128 dimensions and uses float value precision. 
When searching for nearest neighbors in the SIFT vector space we use <em>euclidean</em> distance as our distance metric.</p>

<p>The <em>title_bert_embedding</em> is used to store the  
<a href="https://www.aclweb.org/anthology/D19-1410/">sentence-BERT embedding</a> which has 768 dimensions and we use <em>angular</em> as our distance-metric. 
There are different approaches
for using embeddings from large transformer natural language models like BERT for search but that is out of scope for this document, 
please see the referenced paper for details on how to use BERT as a representation model where query and document text can be encoded independently. 
</p>

<p>We already introduced <em>distance-metric</em> and this metric defines what distance measure is used when searching for the nearest neighbors. 
Currently Vespa supports 4 distance metrics:
<ul>
  <li><b>euclidean</b></li> 
  <li><b>angular</b></li> 
  <li><b>innerproduct</b></li> 
  <li><b>geodegrees</b></li> 
</ul> 

See <a href="reference/schema-reference.html#index-distance-metric">distance-metric reference documentation</a> for details on these <em>distance-metrics</em>. 
</p>
<p>We also need to define the run time <a href="ranking-expressions-features.html#using-query-variables">query tensors</a> in the application package which
are controlled outside of the document schema definition.  

These are the run time tensors which is input to the <em>nearestNeighbor</em> search operator(s).</p>
<pre>
&lt;query-profile-type id="root"&gt;
    &lt;field name="ranking.features.query(query_vector_bert)" type="tensor&amp;lt;float&amp;gt;(x[768])" /&gt;
    &lt;field name="ranking.features.query(query_vector_sift)" type="tensor&amp;lt;float&amp;gt;(x[128])" /&gt;
&lt;/query-profile-type&gt;
</pre>

<p>We also need to configure how to <a href="ranking.html">rank</a> our products. 
We start easy and define a rank profile which rank products by how similar the given query image is in the SIFT vector space:</p>
 
<pre>
  rank-profile image-similarity inherits default {
    first-phase {
      expression: closeness("field", image_sift_encoding) 
    }
  }
</pre>
<p>
As we recall, we configured the tensor <em>distance-metric</em> as <em>euclidean</em> and the <em>closeness("field",image_sift_encoding)</em> Vespa rank feature 
produce the score <em>1/(1+distance)</em> which takes the value 1.0 
if the document vector has distance 0, if the distance is larger than 0 the score will be in the range &lt;0,1]. 
This inversion is done since Vespa sorts documents by decreasing relevancy score and
we want the closest documents to be ranked highest.
The relevancy score is assigned by an expression configured in the ranking profile. 
See details on configuring ranking in Vespa in the <a href="ranking.html">ranking</a> documentation.</p>

The <em>first-phase</em> is part of Vespa's <a href="phased-ranking.html">phased ranking</a> support. 
Phased ranking or multi-stage document ranking enables re-ranking of the top k subset of the hits which are retrieved from the previous stage. 
Highly efficient query
operators like nearest neighbor or <a href="using-wand-with-vespa.html">wand</a> are effectively  zero-phase retrieval as they are both top-k heap based and throws away hits which are 
not making it into the top-K set. This pruning of low ranking hits in the zero-phase retrieval 
reduces the number of hits which gets fully ranked using the <em>first-phase</em> function
and hence reduces latency and cost.</p>

<h3 id="indexing-product-data">Indexing product data</h3>
<p>Once we have deployed the document schema one 
can <a href="writing-to-vespa.html">index</a> the data using the <a href="reference/document-json-format.html">json feed format</a>. 
In the example below we feed two documents using the indexed (dense) tensors short form. Documents feed becomes visible in search immediately (real time indexing). 
</p>
<pre>
[
  {
    "put": "id:shopping:product::998211",
    "fields": {
      "title": "Linen Blend Wide Leg Trousers",
      "in_stock": true,
      "price": 29.95,
      "popularity": 0.342,
      "image_sift_encoding": {
        "values": [
          0.9147281579191466,
          0.5453696694173961,
          0.7529545687063771,
          ..
         ]
      },
       "title_bert_embedding": {
        "values": [
          0.16766378547490635,
          0.3737005826272204,
          0.040492891373747675,
          ..
         ]
        }
      }
  },
  {
    "put": "id:shopping:product::97711",
    "fields": {
      "title": "Loose-fit White Pens",
      "in_stock": false,
      "price": 99.99,
      "popularity": 0.538,
      "image_sift_encoding": {
        "values": [
          0.9785931815169806,
          0.5697209315543527,
          0.5352198004501647,
          ..
        ]
      },
      "title_bert_embedding": {
        "values": [
          0.03515862084651311,
          0.24585168798559187,
          0.6123057708571111,
          ..
        ]
      }
    }
  }
]
</pre>
<p>The above json data can be fed to Vespa using any of the <a href="writing-to-vespa.html#api-and-utilities">Vespa feeding api's</a>. Vespa only supports real time indexing
so there is no explicit batch support and documents become searchable as they are fed to the system with low visibility delay. Tensor fields can be partially updated 
as regular fields (without having to re-index
the entire document).</p>

<h3 id="querying-product-data">Querying product data</h3>

<p>Finally, we are able to start querying using the <a href="reference/query-language-reference.html#nearestneighbor">nearestNeighbor query operator</a>.
The <em>nearestNeighbor</em> query operator expects two arguments; the document tensor field we want to search and the query tensor we want to find
neighbors for. How many neighbors we want is controlled by the <em>targetHits</em> annotation which is a required parameter. 
The query tensor is sent as a query ranking feature
and the query tensor name is referenced in second argument of the nearestNeighbor operator.</p> 

<b>Related Products using Image Similarity</b>
<p>In this example we use the nearest neighbor search operator to recommend similar products based on image similarity. 
For a given image (e.g an product image shown in the product search result page) we can find  
products which have a similar product image. 
We can do this by fetching the SIFT encoding of the image we want to get similar images for 
and use this tensor as input to the nearest neighbor query operator and search over the collection of SIFT encodings.</p>

<p>
We are also not interested in the corpus wide global nearest neighbors but only products where <em>in_stock</em> is <em>true</em>. 
We express the query logic using  
the <a href="query-language.html">YQL query language</a> and our <a href="query-api.html#http">search api request</a> becomes:</p>

<pre>
{
  "yql": "select title, price from sources products where ([{\"targetHits\": 10}]nearestNeighbor(image_sift_encoding,query_vector_sift)) and in_stock = true;",
  "ranking.features.query(query_vector_sift)": [
    0.22507139604882176,
    0.11696498718517367,
    0.9418422036734729,
    ...
  ],
  "ranking.profile": "image-similarity", 
  "hits": 10 
}
</pre>

<p>The YQL express our query logic using logical conjunction <em>and</em> so that our nearest neighbor search is restricted to documents where <em>in_stock</em> is true. 
The <em>targetHits</em> is the desired number of close neighbors we want. 
The total number of hits which is ranked by the ranking profile 
depends on the query filters and how fast the nearest neighbor search algorithm converges. The operator might expose more than 10 hits
to the <em>first-phase</em> ranking expression.  Since we are filtering on <em>in_stock</em> 
one might also get 0 results if the entire product catalog is out of stock. 

The <em>ranking.profile</em> parameter chooses the <em>rank-profile</em> we described earlier which simply ranks documents based on how similar they are in the SIFT 
vector encoding space. 
Finally, the <em>hits</em> parameter controls how many hits are 
returned in the response. If the documents are partitioned over multiple content nodes the <em>targetHits</em> is per node, so with e.g 10 nodes we get at least
100 nearest neighbors fully evaluated using the <em>first-phase</em> ranking function. 

In our example the <em>first-phase</em> expression is simply the <em>closeness()</em> as calculated by the nearest neighbor 
operator and hence the hits are not re-ordered but assigned the same score as calculated in the zero-phase by nearest neighbor search operator.

We could re-order hits by changing the first-phase ranking function, for the following would re-rank the nearest neighbors which are in stock using 
the price:</p>

<pre>
rank-profile image-similarity-price {
  first-phase {
    expression: if(isNan(attribute(price)) == 1,0.0, attribute(price)) 
  }
}
</pre>
<p>In this example the ranking profile simply ignores the closeness score assigned by the nns operator and instead re-orders the 
most similar images by their <em>price</em>, ranked in decreasing order. </p>

<p>
A more advanced example is given below where we retrieve the closest neighbors in SIFT-encoding vector space (with in stock filter)
and ranks the those neighbors using a combination of the euclidean distance rank and the product popularity 
(e.g. calculated offline based on past user behavior with the production) and finally re-ranks the top 100 hits per node 
using a more advanced <a href="onnx.html">ONNX</a> imported Machine Learned (ML) model. These are examples of multi-stage or <a href="phased-ranking.html">phased ranking</a>.
</p>

<pre>
rank-profile image-similarity-with-onnx-stage {
  first-phase {
    expression: closeness("field", image_sift_encoding)*attribute(popularity) 
  }
  second-phase {
    rerank-count: 100
    expression: onnx("image-similarity-model.onnx")
  }
 }
</pre>
<p>With the above multi-stage ranking profiles there are three stages: 
<ul>
<li>The zero-phase nearest neighbor search which only ranks documents by the closeness feature using <em>angular-distance</em></li>
<li>The <em>first-phase</em> which re-ranks the top <em>targetHits</em> from zero-phase</li>
<li>the <em>second-phase</em> which re-ranks the top 100 hits from <em>first-phase</em> ranking stage</li>
</ul>
<p>
<b>Related Products using BERT Text Embedding Similarity</b>
<p>As an alternative to using product image similarity to find related items we look at using the BERT embedding from the product title instead. 
Similar to the image similarity case we assume that we
have the BERT embedding tensor representation for the product we want to find related or similar items for 
and we can use this to search for similar products using the angular distance.  We define our
ranking function:</p>

<pre>
rank-profile title-similarity-using-bert {
  first-phase {
    expression: closeness("field", title_bert_embedding)
  }
}
</pre> 
<p>
Search for 10 nearest neighbors using angular distance in the BERT embedding vector space and we are only interested in products where <em>in_stock</em> is true:
</p>
<pre>
{
  "yql": "select title,price from sources products where ([{\"targetNumHits\": 10}]nearestNeighbor(title_bert_embedding,query_vector_bert)) and in_stock = true;",
  "ranking.features.query(query_vector_bert)": [
    0.7528874147920314,
    0.712266471788029,
    0.8813024904379954,
    ...
  ],
  "ranking.profile": "title-similarity-using-bert",
  "hits": 10
}
</pre>

<b>Related Products using hybrid BERT Text Embedding Similarity</b>
<p> 
We can also combine and retrieve products in a single query request using disjunction (OR) 
logical operator to retrieve products which are close in either vector space and we define 
our ranking function as a weighted linear combination of the closeness scores produced by two nearest neightbor search operators:</p>

<pre>
rank-profile hybrid-similarity {
  first-phase {
    expression: query(bert_weight)*closeness("field", title_bert_embedding) + (1 - query(bert_weight))* closeness("field", image_sift_encoding)
  }
}
</pre>
<p>Our search request need to pass both the BERT embedding tensor and the SIFT encoding tensor and we use logical disjunction (OR) to combine the
two nearest neighbor search operators, our search request becomes: </p>
<pre>
{
  "yql": "select title, price from sources products where (([{\"targetNumHits\": 10}]nearestNeighbor(title_bert_embedding,query_vector_bert)) 
or ([{\"targetNumHits\": 10}]nearestNeighbor(image_sift_encoding,query_vector_sift))) and in_stock = true;",
  "hits": 10,
  "ranking.features.query(query_vector_sift)": [
    0.5158057404746615,
    0.6441591144976925,
    0.24075880767944935,
    ...
  ],
  "ranking.features.query(query_vector_bert)": [
    0.708719978302217,
    0.4800850502044147,
    0.03310770782193717,
    ... 
  ],
  "ranking.features.query(bert_weight)": 0.1,
  "ranking.profile": "hybrid-similarity"
}
</pre>
We ask for 10 nearest neighbors in the title BERT embedding vector space or 10 nearest neighbors in the SIFT encoding space 
which in total will expose at least 20 documents to the 
<em>first-phase</em> ranking function. 

<h3 id="hybrid-neural-term-retrieval">Hybrid Neural Term Retrieval</h3>
<p>In the following example we have a user input query (for example 'white trousers for woman') and we search for both products which matches 
the query terms and for the nearest neighbors using the neural BERT embedding space.  In this case we need to produce the BERT embedding 
of the input text query and we use logical disjunction, this allows also retrieving documents which does not use the exact same vocabulary as 
in the user query. Our search request becomes: </p>
<pre>
{
  "yql": "select title, price from sources products where in_stock = true and (userQuery() or ([{\"targetNumHits\": 10}]nearestNeighbor(title_bert_embedding,query_vector_bert)));",
  "hits": 10,
  "ranking.features.query(query_vector_bert)": [
    0.10209943251234721,
    0.6693870055804121,
    0.5088424671764654,
    ...
  ],
  "ranking.profile": "hybrid-term-neural-retrieval",
  "query": "white pants for women",
  "type": "any"
}

</pre>
<p>The query will retrieve products based on the title bert embedding similarity and term based matching. We use the following ranking profile:</p>
<pre>
  rank-profile hybrid-term-neural-retrieval {
    first-phase {
      expression: 0.5*fieldMatch(title) + 0.5*closeness("field", title_bert_embedding)
    }
  }
</pre>
<p>The retrieved hits are ranked using a linear combination of the closeness 
and the <a href="/reference/string-segment-match.html">fieldMatch</a> which is a native built in Vespa
text matching feature which also takes term proximity into account when computing the text ranking score.</p>

<h3 id="programatic-usage">Using Nearest Neighbor from a Searcher Component</h3>
<p>As with all query operators in Vespa one also build the query tree programmatically 
in a custom <a href="searcher-development.html">Searcher component</a>.</p>
<p>In the example below we execute 
the incoming query and fetch the Tensor of the top ranking hit ranked by whatever rank-profile was specified in the search request.
We read the <em>image_sift_encoding</em> Tensor from the top ranking hit and uses it as the input for the nearest neighbor search operator to find 
related products (Using the rank profile described in earlier sections).</p>

<pre>
package ai.vespa.example.searcher;

import com.yahoo.prelude.query.NearestNeighborItem;
import com.yahoo.search.Query;
import com.yahoo.search.Result;
import com.yahoo.search.Searcher;
import com.yahoo.search.result.Hit;
import com.yahoo.search.searchchain.Execution;
import com.yahoo.tensor.Tensor;

public class FeedbackSearcher extends Searcher {
    
    @Override
    public Result search(Query query, Execution execution) {
        Result result = execution.search(query);
        if (result.getTotalHitCount() == 0)
            return result;
        execution.fill(result); //fetch summary data of top hits
        Hit bestHit = result.hits().get(0);
        Tensor sift_encoding = (Tensor) bestHit.getField("image_sift_encoding");
        Query relatedQuery = new Query();
        relatedQuery.getRanking().setProfile("image-similarity");
        relatedQuery.getRanking().getFeatures().put("query(query_vector_sift)", sift_encoding);
        NearestNeighborItem nn = new NearestNeighborItem(
                "image_sift_encoding",
                "query_vector_sift");
        nn.setTargetNumHits(10);
        relatedQuery.getModel().getQueryTree().setRoot(nn);
        return execution.search(relatedQuery);
    }
}
</pre>
