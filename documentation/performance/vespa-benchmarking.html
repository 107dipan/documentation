---
# Copyright 2017 Yahoo Holdings. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Vespa Benchmarking"
---

<p>
Benchmarking a Vespa application is essential to get an idea of
how well the test configuration performs.
Thus, benchmarking is an essential part of sizing a search cluster itself.
Benchmarking a cluster can answer the following questions:
<ul>
  <li>What throughput and latency can you expect from a search node?</li>
  <li>Which resource is the bottleneck in the system?</li>
</ul>
These in turn indirectly answers other questions such as how many nodes are needed,
and if it will help to upgrade your disk or CPU.
Thus, benchmarking will help in finding the optimal Vespa configuration,
using all resources optimally, which in turn lowers costs.
But when should one benchmark?
A good rule is to benchmark whenever the workload changes.
Benchmarking should also be done when adding new features to queries.
</p><p>
Before benchmarking, consider:
<ul>
<li>What is the expected query mix?
    Having a representative query mix to test with is essential in order to get valid results.
    Splitting up in different types of queries
    is also a useful way to get an idea of which query is the heaviest.</li>
<li>What is the expected SLA, both in terms of latency and throughput?</li>
<li>How important is real-time behavior? What is the rate of incoming documents, if any?</li>
</ul>
Having an understanding of the query mix and SLA will help setting the test parameters.
</p>



<h2 id="vespa-fbench">vespa-fbench</h2>
<p>
Vespa provides a query load generator tool,
<a href="../reference/vespa-cmdline-tools.html#vespa-fbench">vespa-fbench</a>,
to run queries and generate statistics - much like a traditional web server load generator.
It allows running any number of <em>clients</em>
(i.e. the more clients, the higher load), for any length of time,
and adjust the client response time before issuing the next query.
It outputs the throughput, max, min, and average latency,
as well as the 25, 50, 75, 90, 95 and 99 percentiles.
This provides quite accurate information of how well the system manages the workload.
</p><p>
Note: It is possible to list several hostnames and ports.
The different hostnames will be distributed to the clients in a round-robin manner,
such that, with two hosts, client 0, 2, &hellip;, 38 makes requests to the first host,
while client 1, 3, &hellip;, 39 makes requests to the second host.
</p>


<h3 id="preparing-queries">Preparing queries</h3>
<p>
vespa-fbench uses <em>query files</em> for GET and POST queries -
see the <a href="../reference/vespa-cmdline-tools.html#vespa-fbench">reference</a> - examples:
<!-- ToDo: make more complex examples than the reference -->
<pre>
/search/?yql=select%20%2A%20from%20sources%20%2A%20where%20sddocname%20contains%20%22music%22%3B
</pre>
<pre>
/search/
{"yql" : "select * from sources * where sddocname contains music;"}
</pre>
A common way to make query files is to use the queries from production installations,
or generate the queries from the document feed or expected queries.
</p><p>
<em>vespa-fbench</em> runs each client in a separate thread.
Split the query files into one file per client:
<pre>
$ gunzip -c queries.gz | grep '/search/' | <a href="../reference/vespa-cmdline-tools.html#vespa-fbench-split-file">vespa-fbench-split-file</a> 512
</pre>
Make sure to generate as many files as concurrent clients (<code>-n</code> parameters) -
use <code>-r</code> to reuse query files.
Each client would use a query and output file given by the given pattern and it's client number,
i.e. client 1 will use query file query001.txt and output file output001.txt.
</p>


<h3 id="run-queries">Run queries</h3>
<p>
A typical vespa-fbench command looks like:
<pre>
$ vespa-fbench -n 8 -q query%03d.txt -s 300 -c 0 myhost.mydomain.com 8080
</pre>
This starts 8 clients, each using queries from a query file
prefixed with <code>query</code>, followed by the client number.
This way, client 1 will use <code>query000.txt</code> etc.
The <code>-s</code> parameter indicates that the benchmark will run for 300 seconds.
The <code>-c</code> parameter, states that each
client should wait for 0 milliseconds between each query.
This enables you to control user interactivity.
The last two parameters are container hostname and port.
Multiple hosts and ports may be provided,
and the clients will be uniformly distributed to query the containers round robin.
</p><p>
A more complex example, using docker, hitting a Vespa Cloud endpoint:
<pre>
$ docker run -v /Users/myself/tmp:/testfiles \
      -w /testfiles --entrypoint '' vespaengine/vespa \
      /opt/vespa/bin/vespa-fbench \
          -C data-plane-public-cert.pem -K data-plane-private-key.pem -T /etc/ssl/certs/ca-bundle.crt \
          -n 10 -q query%03d.txt -o output%03d.txt -xy -s 300 -c 0 \
          myapp.mytenant.aws-us-east-1c.public.vespa.oath.cloud 443
</pre>
</p>



<h3 id="post-processing">Post Processing</h3>
<p>
After running vespa-fbench you will have a summary written to stdout
(and possibly an output file from each client) - example output:
<pre>
***************** Benchmark Summary *****************
clients:                      30
ran for:                    1800 seconds
cycle time:                    0 ms
lower response limit:          0 bytes
skipped requests:              0
failed requests:               0
successful requests:    12169514
cycles not held:        12169514
minimum response time:      0.82 ms
maximum response time:   3010.53 ms
average response time:      4.44 ms
25 percentile:              3.00 ms
50 percentile:              4.00 ms
75 percentile:              6.00 ms
90 percentile:              7.00 ms
95 percentile:              8.00 ms
99 percentile:             11.00 ms
actual query rate:       6753.90 Q/s
utilization:               99.93 %
</pre>
Take note of the number of <em>failed requests</em>,
as a high number here can indicate that the system is overloaded,
or that the queries are invalid.
</p><p>
The options <code>-xy</code> makes vespa-fbench clients output
benchmarking data to it's output files.
Note that saving all responses to disk might impact the performance of
the benchmarking itself. If only the summary is needed it is
recommended to not use output files.
</p><p>
There are also tools to format the vespa-fbench output into something more
manageable for plotting.
<a href="../reference/vespa-cmdline-tools.html#vespa-fbench-result-filter.pl">
vespa-fbench-result-filter.pl</a> formats the above output into a space-separated format.
</p><p>
Notes: <!-- ToDo: clean this up -->
<ul>
  <li>
    'system utilization' provides no information about the Vespa
  system under test and should not be used for benchmarking
  </li><li>
    In some modes of operation, vespa-fbench waits before sending the next query
  </li><li>
    'system utilization' represents the time that vespa-fbench is
    sending queries and waiting for responses. For example,
    a 'system utilization' of 50% means that vespa-fbench is stress testing the system 50% of the time,
    and is doing nothing the remaining 50% of the time
  </li><li>
    Do not run vespa-fbench on the same machine as the container
    or the search node because it will impact the performance of vespa system
  </li><li>
    vespa-fbench latency results include network latency.
    Measure and subtract network latency to obtain the true vespa query latency.
    See also <a href="../reference/vespa-cmdline-tools.html#vespa-fbench">vespa-fbench -xy</a> options
  </li><li>
    If many of the queries return zero results, the average latency will be low</li>
</ul>
</p>
