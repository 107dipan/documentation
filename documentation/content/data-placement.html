---
# Copyright 2017 Yahoo Holdings. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Document Distribution"
---

<p>
Documents can be uniformly distributed or in groups.
Read <a href="../elastic-vespa.html">elastic Vespa</a> before this article.
</p><p>
The <a href="../reference/services-content.html#group">group</a> element is used to distribute
<a href="buckets.html">buckets</a> of documents - use cases:
<table class="table">
<thead></thead><tbody>
<tr>
  <th style="white-space: nowrap">Cluster upgrade</th>
  <td>
    Use two (or more) groups  - then shut down one group in the cluster for upgrade.
    This is quicker than doing it one by one node -
    the tradeoff is that 50% of the nodes must sustain the full load (in case of two groups),
    and one must always grow the cluster by two (or more) nodes, one in each group.
    Latency per search node goes up with a larger index, such grouping will double the index size.
  </td>
</tr><tr>
  <th style="white-space: nowrap">Query throughput</th>
  <td>
    Applications with high query load on a small index can benefit of keeping the full index on all nodes -
    i.e. configuring the same number of groups as nodes.
    This confines queries to one node only,
    so less <em>total</em> work is done per query in the cluster.
    Hence the throughput increase.
    This increases latency compared with splitting the index on many nodes.
    It also limits options when growing the index size -
    at some point, the query latency might grow too high,
    and the only option then is to add nodes to the group.
    Having two nodes per group is bad if one node goes down,
    as one node must hence be able to hold the full index again.
  </td>
</tr><tr>
  <th style="white-space: nowrap">Cautious data placement</th>
  <td>
    A way to reduce the risk of data unavailability,
    is to spread the data copies across different locations,
    like placing all the copies in different racks.
    Furthermore, this data placement enables fast upgrade procedures without service interruption,
    as entire groups can be upgraded at a time.
  </td>
</tr>
</tbody>
</table>
By default, Vespa distributes <a href="buckets.html">buckets</a>
uniformly across all distributor and storage nodes.
Use hierarchical distribution to control where bucket copies are stored in relation to each other.
For instance, if your cluster consists of 10 racks,
you can define that for any given bucket,
it should store 2 copies in one rack and 2 copies in another rack.
That means that you can take down a whole rack,
and you are still sure that all your data is available,
because you know all data on that rack should have 2 copies on another rack.
</p><p>
Also, improved network bandwidth can be utilized.
If a node has restarted and it needs to fetch some data that came in while restarting to get up to date,
it can now choose to fetch that data from a node on the same rack,
which has better connectivity as it is in the same rack.
(Here you must of course define one group per rack,
and add the nodes in the rack in the respective groups)
</p><p>
When configuring the groups, specify how data is to be distributed among the group children.
Refer to the <a href="../reference/services-content.html#group">group</a>
documentation for details about how to configure storage groups.
</p><p>
Which groups are selected as primary, secondary and so on, groups for a
given bucket is randomly determined using the same ideal state algorithm we
use to pick nodes, described in more detail in the
<a href="idealstate.html">ideal state</a> reference document.
Each group is assigned an index, to be used in this algorithm.
This will however likely create a bit worse skew globally compared to not using groups.
</p>


<h2 id="migrate-to-using-groups-example-1">Migrate to using groups - example 1</h2>
<p>
Migrating from flat distribution to a hierarchy will temporarily reduce query coverage.
To work around, increase redundancy first, then reconfigure to using groups.
When adding new group(s), it will take some time to populate the new nodes.
During this transition period, queries hitting these nodes will return partial results.
<a href="../reference/services-content.html#nodes">Reference documentation</a>.
</p><p>
Example - migrate to using groups. Starting point:
A cluster with 4 nodes and flat distribution and redundancy 2.
End state: all 4 nodes with all documents:
<pre>
&lt;content id="music" version="1.0"&gt;
  &lt;redundancy&gt;2&lt;/redundancy&gt;
  &lt;documents&gt;
    &lt;document mode="index" type="music" /&gt;
  &lt;/documents&gt;
  &lt;nodes&gt;
    &lt;node hostalias="node0"/&gt;
    &lt;node hostalias="node1"/&gt;
    &lt;node hostalias="node2"/&gt;
    &lt;node hostalias="node3"/&gt;
  &lt;/nodes&gt;
  &lt;engine&gt;
    &lt;proton&gt;
      &lt;searchable-copies&gt;2&lt;/searchable-copies&gt;
    &lt;/proton&gt;
  &lt;/engine&gt;
&lt;/content&gt;
</pre>
Increase <em>redundancy</em> and <em>searchable-copies</em> to 4.
After deploying this, wait for all search nodes to finish redistribution
(i.e. all nodes have all documents):
<pre>
&lt;content id="music" version="1.0"&gt;
  &lt;redundancy&gt;4&lt;/redundancy&gt;
  &lt;documents&gt;
    &lt;document mode="index" type="music" /&gt;
  &lt;/documents&gt;
  &lt;nodes&gt;
    &lt;node hostalias="node0"/&gt;
    &lt;node hostalias="node1"/&gt;
    &lt;node hostalias="node2"/&gt;
    &lt;node hostalias="node3"/&gt;
  &lt;/nodes&gt;
  &lt;engine&gt;
    &lt;proton&gt;
      &lt;searchable-copies&gt;4&lt;/searchable-copies&gt;
    &lt;/proton&gt;
  &lt;/engine&gt;
&lt;/content&gt;
</pre>
Reconfigure to 4 groups, setting <em>redundancy</em>
and <em>searchable-copies</em> to 1 (these settings apply per group):
<pre>
&lt;content id="music" version="1.0"&gt;
  &lt;redundancy&gt;1&lt;/redundancy&gt;
  &lt;documents&gt;
    &lt;document mode="index" type="music" /&gt;
  &lt;/documents&gt;
  &lt;group distribution-key="0" name="mytopgroup"&gt;
    &lt;distribution partitions="1|*"/&gt;
    &lt;group distribution-key="0" name="mygroup0"&gt;
      &lt;node distribution-key="0" hostalias="node0"/&gt;
    &lt;/group&gt;
    &lt;group distribution-key="1" name="mygroup1"&gt;
      &lt;node distribution-key="0" hostalias="node1"/&gt;
    &lt;/group&gt;
    &lt;group distribution-key="2" name="mygroup2"&gt;
      &lt;node distribution-key="0" hostalias="node1"/&gt;
    &lt;/group&gt;
    &lt;group distribution-key="3" name="mygroup3"&gt;
      &lt;node distribution-key="0" hostalias="node1"/&gt;
    &lt;/group&gt;
  &lt;/group&gt;
  &lt;engine&gt;
    &lt;proton&gt;
      &lt;searchable-copies&gt;1&lt;/searchable-copies&gt;
    &lt;/proton&gt;
  &lt;/engine&gt;
&lt;/content&gt;
</pre>
</p>


<h2 id="migrate-to-using-groups-example-2">Migrate to using groups - example 2</h2>
<!-- ToDo: Reference to performance/sizing-search.html and/or migrate this there -->
<p>
The following example has a simple content cluster with flat distribution,
redundancy 2 and 3 content/search nodes:
<pre>
&lt;content version="1.0" id="mycluster"&gt;
  &lt;redundancy&gt;2&lt;/redundancy&gt;
  &lt;documents&gt;
    &lt;document mode="index" type="mydoctype"/&gt;
  &lt;/documents&gt;
  &lt;group distribution-key="0" name="mytopgroup"&gt;
    &lt;node distribution-key="0" hostalias="node0"/&gt;
    &lt;node distribution-key="1" hostalias="node1"/&gt;
    &lt;node distribution-key="2" hostalias="node2"/&gt;
  &lt;/group&gt;
  &lt;engine&gt;
    &lt;proton&gt;
      &lt;searchable-copies&gt;2&lt;/searchable-copies&gt;
    &lt;/proton&gt;
  &lt;/engine&gt;
&lt;/content&gt;
</pre>
Lets say we need to increase the QPS in our system by a factor of 2
and that the static query cost (SQC) is high.
We cannot just add 3 more content/search nodes to reach the desired QPS.
Instead we use hierarchical distribution to get extra groups
that also contain the entire document collection.
The next section shows how to setup such a system.
</p><p>
We have defined 2 leaf groups (<em>mygroup0</em>, <em>mygroup1</em>)
that are located under the top group.
Each leaf group has 3 content/search nodes.
The total redundancy of the system is now 4,
and the <a href="content/data-placement.html">distribution partitions</a>
is specified such that each leaf group will have 2 copies of the documents in the system.
Each query can be sent to the nodes of a single group instead of all nodes in the system
and the QPS requirement can be met through parallelization of searches.
With redundancy 2 per leaf group we can also lose a node from a group
and still have enough coverage.
<pre>
&lt;content version="1.0" id="mycluster"&gt;
  &lt;redundancy&gt;4&lt;/redundancy&gt;
  &lt;documents&gt;
    &lt;document mode="index" type="mydoctype"/&gt;
  &lt;/documents&gt;
  &lt;group distribution-key="0" name="mytopgroup"&gt;
    &lt;distribution partitions="2|*"/&gt;
    &lt;group distribution-key="0" name="mygroup0"&gt;
      &lt;node distribution-key="0" hostalias="node0"/&gt;
      &lt;node distribution-key="1" hostalias="node1"/&gt;
      &lt;node distribution-key="2" hostalias="node2"/&gt;
    &lt;/group&gt;
    &lt;group distribution-key="1" name="mygroup1"&gt;
      &lt;node distribution-key="3" hostalias="node3"/&gt;
      &lt;node distribution-key="4" hostalias="node4"/&gt;
      &lt;node distribution-key="5" hostalias="node5"/&gt;
    &lt;/group&gt;
  &lt;/group&gt;
  &lt;engine&gt;
    &lt;proton&gt;
      &lt;searchable-copies&gt;4&lt;/searchable-copies&gt;
    &lt;/proton&gt;
  &lt;/engine&gt;
&lt;/content&gt;
</pre>
</p>

