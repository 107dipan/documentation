---
# Copyright 2017 Yahoo Holdings. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Content nodes and states"
---

<p>
Refer to <a href="../elastic-vespa.html#writing-documents">writing documents</a>
for an overview of the Document API flow using the nodes described below.
</p><p>
Refer to <a href="../operations/admin-procedures.html">administrative procedures</a>
for configuration and state monitoring / management.
</p>
<img src="../img/design/clustercontroller-overview.png"/>
<!-- ToDo: replace with better illustration -->



<h2 id="node-state">Node state</h2>
<p>
Content and distributor nodes have state:
</p>
<table class="table">
<tr>
  <th>Up</th>
  <td>The node is up and available to keep buckets and serve requests.</td>
</tr><tr>
  <th>Down</th>
  <td>The node is not available, and can not be used.</td>
</tr><tr>
  <th>Initializing</th>
  <td>The node is starting up.
      It knows what buckets it stores data for, and may serve requests,
      but it does not know the metadata for all its buckets yet,
      such as checksums and document counts.
      The node is available for bucket placement.
  </td>
</tr><tr>
  <th>Stopping</th>
  <td>This node is stopping and is expected to be down soon.
      This state is typically only exposed to the cluster controller
      to tell why the node stopped.
      The cluster controller will expose the node as down
      or in maintenance mode for the rest of the cluster.
      This state is thus not seen by the distribution algorithm.
  </td>
</tr><tr>
  <th>Maintenance</th>
  <td>This node is temporarily unavailable.
      The node is available for bucket placement, redundancy is hence lower. <!-- ToDo rewrite -->
      Using this mode, new copies of the documents stored on this node will not be created,
      allowing the node to be down with less of a performance impact on the rest of the cluster.
      This mode is typically used to mask a down state during controlled node restarts,
      or by an administrator that need to do some short maintenance work,
      like upgrading software or restart the node.
  </td>
</tr><tr>
  <th>Retired</th>
  <td>A retired node is available and serves requests.
      This state is used to remove nodes while keeping redundancy.
      Buckets are moved to other nodes (with low priority), until empty.
      Special considerations apply when using
      <a href="../elastic-vespa.html#grouped-distribution">grouped distribution</a>
      as buckets are not necessarily removed.
  </td>
</tr>
</table>
<p>
Distributor nodes start / transfer buckets quickly
and are hence not in <em>maintenance</em> or <em>retired</em>.
<!-- ToDo: it should not be possible to set a distributor in this mode, check -->
</p>



<h2 id="cluster-state">Cluster state</h2>
<p>
There are three kinds of state:
</p>
<table class="table">
<tr id="unit-state">
  <th>Unit state</th>
  <td>
  <p>
  The cluster controller fetches states from all nodes, called <em>unit states</em>.
  States reported from the nodes are either <em>initializing</em>, <em>up</em> or <em>stopping</em>.
  If the node can not be reached, a <em>down</em> state is assumed.
  </p>
  </td>
</tr><tr id="user-state">
  <th>User state</th>
  <td>
  <p>
  <em>user state</em> must be one of
  <em>up</em>, <em>down</em>, <em>maintenance</em> or <em>retired</em>:
  <ul>
  <li>Retire a node from a cluster -
      use <em>retired</em> to move buckets to other nodes</li>
  <li>Short-lived maintenance work -
      use <em>maintenance</em> to avoid merging buckets to other nodes</li>
  <li>Fail a bad node. The cluster controller or an operator can set a node <em>down</em></li>
  </ul>
  </p>
  </td>
</tr><tr id="generated-state">
  <th style="white-space: nowrap">Generated state</th>
  <td>
  <p>
  The cluster controller <em>generates</em> the cluster state
  from the <em>unit</em> and <em>user</em> states, over time.
  </p><p>
  The cluster controller will typically mask a <em>down</em> unit state with a
  <em>maintenance</em> state for a short time if it has seen a stopping state,
  indicating a controlled restart (assuming the node will soon be initializing again)
  If the node fails to come back quickly, the state is set down.
  </p><p>
  <em>down</em> nodes that come back up, reporting as <em>initializing</em>,
  is masked as <em>down</em> until the node reports <em>up</em>.
  (The cluster controller might suspect it of repeatedly stalling or crashing during initialization,
  and may keep it <em>down</em> to avoid the cluster to be interrupted) <!-- ToDo rewrite -->
  </p>
  </td>
</tr>
</table>
<p>
For new cluster states, the cluster state version is upped,
and the new cluster state is broadcasted to all nodes.
There is a minimum time between each cluster state change.
<!-- ToDo ref to config here -->
</p><p>
It is possible to set a minimum capacity for the cluster state to be <em>up</em>.
<!-- ToDo ref to config here -->
If a cluster has so many nodes unavailable that it is considered down,
the state of each nodes is irrelevant,
and thus new cluster states will not be created and broadcasted
before enough nodes are back for the cluster to come back up.
A cluster state indicating the entire cluster is down,
may thus have outdated data on the node level.
</p>
<!-- ToDo: An example cluster state string is useful here -->



<h2 id="cluster-controller">Cluster controller</h2>
<p>
The main task of the cluster controller is to maintain the <a href="#cluster-state">cluster state</a>.
This is done by <em>polling</em> nodes for state,
<em>generating</em> a cluster state,
which is then <em>broadcast</em> to all the content nodes in the cluster:
<table class="table">
<tr id="node-state-polling">
  <th>Node state polling</th>
  <td>
  <p>
  The cluster controller polls nodes, sending the current cluster state.
  If the cluster state is no longer correct, the node returns correct information immediately.
  If the state is correct, the request lingers on the node,
  such that the node can reply to it immediately if its state changes.
  After a while, the cluster controller will send a new state request to the node,
  even with one pending.
  This triggers a reply to the lingering request and makes the new one linger instead.
  Hence, nodes have a pending state request.
  </p><p>
  During a controlled node shutdown, it starts the shutdown process
  by responding to the pending state request that it is now stopping.
  <strong>Note: </strong> As controlled restarts or shutdowns are implemented as TERM signals
  from the <a href="../config-sentinel.html">config-sentinel</a>,
  the cluster controller is not able to differ between controlled and other shutdowns.
  </p>
  </td>
</tr><tr id="cluster-state-generation">
  <th>Cluster state generation</th>
  <td>
  <p>
  The node states in the cluster state do not necessarily match the states
  reported from the node themselves.
  E.g, a node may return that it is <em>stopping</em>,
  but the cluster state will say that this node is <em>down</em> or in <em>maintenance</em>.
  </p><p>
  The cluster controller is also free to keep the node temporarily out of service.
  For instance, if the node died while initializing last time it started,
  the cluster controller may keep the node in the down state
  until it verifies that the node managed to finish initializing this time around.
  </p><p>
  On top of this, there may be permanent state overrides for the node.
  If the cluster controller detects the node constantly restarting,
  creating unnecessary state flux that interrupts the cluster,
  it may permanently set the node out of service until an administrator can fix it.
  Administrators may also manually override the state to set the node temporarily out of service
  while doing maintenance work, or removing it permanently due to some issues being manually detected.
  </p><p>
  The node states in the cluster states are thus calculated based on the
  reported states from the node, and historic knowledge kept in the cluster controller.
  If the result ends up in a state with higher availability than the user state for the node,
  the state is adjusted based on it.
  </p>
  </td>
</tr><tr id="state-broadcast">
  <th>Cluster state broadcasting</th>
  <td>
  <p>
  When node states are received, a cluster controller internal cluster state is updated.
  To limit the amount of cluster states broadcasted to the cluster,
  the cluster controller is not allowed to create a new official state before a
  given time period has past since the last one.
  Additionally it will wait for a short period of time after receiving a change,
  in case there are other changes happening at about the same time that should be included too.
  For instance, distributors and content nodes that are on the same node
  often stop at the same time.
  </p><p>
  If the internal cluster state has changed enough from the last official
  cluster state for the cluster to care,
  enough time has passed since the last official state was generated (typically a few seconds),
  and no more node state changes have been seen recently (typically less than a second),
  a new official cluster state is generated based on the internal state.
  The version number is upped, and the new official cluster state is immediately broadcasted to all the nodes.
  </p>
  </td>
</tr>
</table>
Note that clients do not interface with the cluster controller -
they get the cluster state from the distributors - <a href="#distributor">details</a>.
</p><p>
The <em>cluster controller</em> will:
<table class="table">
<thead></thead><tbody>
<tr>
  <th>Give the cluster a uniform view of its state</th><td>
    The main cluster controller task is to give the cluster a uniform view
    of what nodes are available and what they capacities are.
    To do this, it polls all distributor and content nodes for health and state information.
    All the states are combined into a cluster state which is again broadcasted.
  </td>
</tr><tr>
  <th>Allow administrators to override node states</th><td>
    If one wants a node to be less available than its health indicates,
    administrators can override node states in the cluster controller.
    Maintenance and retired modes are options to stop using nodes
    other than just stopping the services.
    Enforcing a node to be down also means that
    if someone accidentally starts the services again,
    it will not enter the live cluster.
  </td>
</tr><tr>
  <th>Automatically detect and fix cluster issues</th><td>
    The cluster controller may notice that a node is failing,
    such as constantly crashing.
    The cluster controller can detect this and stop using problematic nodes.
  </td>
</tr><tr>
  <th>Improving alerting and monitoring</th><td>
    As the cluster controller monitors content nodes' health,
    and keeps some node metrics,
    it is a good location to implement utilities
    to improve automatic detection of issues and cluster monitoring.
  </td>
</tbody>
</table>
</p>


<h3 id="master-election">Master election</h3>
<p>
A set of cluster controllers elect a master among themselves.
The master gathers <a href="#node-state">node states</a>
from all the distributor and content nodes, and combine them into a cluster state.
The cluster state is broadcasted to all the nodes every time it changes,
allowing all nodes to know what distributors are responsible for handling what parts of the data.
Clients picks up an updated cluster state if they try talking to the wrong distributor.
</p><p>
Having only a single cluster controller, if the controller goes down for any reason,
there is no longer a cluster controller alive to adjust the cluster state.
While this is not a single point of failure,
as typically a distributor or content node have to go down afterwards for it to matter much,
it would still be good if one could have more than one cluster controller
such that it was less critical for one to go down.
</p><p>
However, having two cluster controllers working at the same time,
the cluster could end up confused with the cluster controllers sending contradicting cluster states.
One of them may have failed to talk to node 3 while the other one may have failed to talk to node 5,
and then they send out two different cluster states.
The nodes in the cluster may receive the cluster states in different order,
and may end up confused with the state of the cluster.
</p><p>
To allow multiple controllers, but avoid confusion,
only a single cluster controller is allowed to broadcast cluster states.
This cluster controller is known as the master cluster controller.
The other cluster controller instances only exist to participate in master election
and potentially take over if the master dies.
</p><p>
All cluster controllers will vote for cluster controller with the lowest index that says it is ready.
If a cluster controller have more than half of the configured cluster controllers voting for it,
it will be elected master.
A newly elected master will not start broadcasting states before a transition time is passed,
allowing an old master to have some time to realize it is no longer the master.
</p><p>
This means, that for there to be a master with <code>N</code> nodes down,
<code>2 * N + 1</code> cluster controllers must be configured.
It also means only the cluster controllers with the <code>N</code> lowest indexes
will ever be able to become master.
The remaining cluster controllers will never do anything
but participate in the master election voting process.
</p>


<h3 id="historic-state">Historic state</h3>
<p>
Very little state is persisted by the cluster controllers.
If the master changes, or is restarted, historic knowledge built up is typically reset.
A few exceptions exist, being stored in a ZooKeeper cluster:
<ul>
  <li>The last cluster state version is stored, such that a cluster controller
      taking over can continue the version numbers where the old one left of.</li>
  <li>The user states set externally are stored, such that a reset does not
      remove an administrator to set a node down for instance.</li>
</ul>
Note: ZooKeeper data tends to be corrupted if the disk they store data on gets full.
It is thus recommended to not store ZooKeeper data on partitions
that are also used for logs or data storage, which are prone to getting filled.
If ZooKeeper data is corrupted, it must be deleted,
in which case all persisted state is cleared.
If having multiple nodes, the state may survive from a node where the data is not corrupt.
Otherwise the data is lost. User states must be reset.
Cluster state versions will start back at zero.
</p>


<h3 id="version-reset">Cluster State Version Reset</h3>
<p>
If ZooKeeper data goes missing, the cluster state version will be reset to zero.
This is not an issue for distributors and content nodes,
as these will trust that the most recently received cluster state is always the most correct one.
</p><p>
Clients use the versions to detect that a cluster state is upgraded.
The cluster controller tries to broadcast the cluster state at the same time to all the nodes,
but the distributors may process the new cluster state at a bit different times,
such that clients may alternatively talk to distributors having the newest cluster state or not.
By using the version numbers, the clients does not dump the newest cluster state
for one received by a distributor that have not processed the new one yet.
</p><p>
To handle a cluster state version reset,
clients have some functionality for detecting whether distributors seems to agree
the cluster state it has is wrong, even if version is newer.
In such cases clients will dump its cluster state and use a new one even if version is lower.
Clients thus handle a version reset,
but it may result in a few temporarily failures while it is confused.
</p>



<h2 id="distributor">Distributor</h2>
<p>
The distributor keeps track of which search nodes should hold which copy of each bucket,
based on the <em>redundancy</em> setting and information from the cluster controller.
Distributors are responsible for keeping track of metadata for a non-overlapping subset of the data in the cluster.
The distributors each keep a bucket database containing metadata of buckets it is responsible for.
This metadata indicates what content nodes store copies of the buckets,
the checksum of the bucket content and the number of documents and meta entries within the bucket.
Each incoming document is assigned to a bucket and forwarded to the right search nodes.
All requests related to these buckets are sent through the distributors.
</p><p>
A <a href="../elastic-vespa.html#bucket-distribution-algorithm">distribution algorithm</a>
calculates what distributors are responsible for given buckets based on
information of what nodes are available in the cluster.
Clients calculate what distributor to talk to using information in the cluster state.
</p><p>
When a distributor goes down or becomes available,
all distributors reshuffle what buckets they are responsible for.
During this reshuffling, clients may get some requests temporarily failing
as distributors taking over bucket ownership doesn't yet know where copies of its buckets are located.
These requests will automatically be resent, providing timeouts allow for it.
Changes in what distributors are up and available will currently cause windows of availability loss.
Other distributors take over bucket ownership.
To do this, they fetch bucket metadata for new buckets from all storage nodes,
in-memory operations for speed.
</p><p>
The distribution algorithm also calculates what content nodes copies of buckets should be stored on.
As moving bucket copies take time, the distributors track current state in a bucket database,
allowing requests to be processed independent of where buckets are currently located.
Distributors use the distribution algorithm to detect buckets that are stored on wrong nodes,
and move copies to correct nodes when there are free resources to do so.
</p><p>
In addition the distributor can track some historic knowledge.
For instance it may know that of the currently existing bucket copies,
a given copy has been available all the time and may be trusted to have all the information,
while another copy may be on a storage node that recently restarted,
so that copy may lack some documents.
Such historic state is not persisted, and is thus lost on distributor restarts.
</p><p>
The distributors use the bucket metadata to ensure the buckets are in a good state.
<ul>
  <li>If buckets have too few copies, new copies are generated</li>
  <li>If buckets have too many copies, superfluous copies are deleted once the
    distributor knows the copies to delete don't contain data other copies do not have</li>
  <li>If all the copies do not contain the same data,
    merges are issued to get bucket copies consistent</li>
  <li>If two buckets exist, such that both may contain the same document,
    the buckets are split or joined to remove such overlapping buckets</li>
  <li>If buckets contain too little or too much data, they should be joined or split</li>
  <li>If not exactly one copy is marked active, and the backend wants a copy to
    be marked active, activate or deactivate copies to get in a good state</li>
</ul>
The maintenance operations have different priorities.
The distributor prioritizes to fix the most critical issues first.
If no maintenance operations are needed, the cluster is said to be in the <em>ideal state</em>.
The distributors synchronize maintenance load with user load,
e.g. to remap requests to other buckets after bucket splitting and joining.
</p><p>
<!-- ToDo rewrite this -->
Clients use the cluster state to route to the correct distributor
using the <a href="idealstate.html">ideal state algorithm</a>.
They get updated cluster states through the distributors they talk to.
If clients use the wrong distributor for a bucket,
it will get the correct cluster state returned in the reply.
The client then calculates the correct distributor and resends the request.
That way, clients do not depend directly on the cluster controller
as distributors serve cluster states to clients.
</p>
