---
# Copyright 2017 Yahoo Holdings. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Content nodes and states"
---

<p>
Refer to <a href="../operations/admin-procedures.html">administrative procedures</a>
for configuration and state monitoring / management.
</p><p>
Refer to <a href="../elastic-vespa.html#writing-documents">writing documents</a>
for an overview of the Document API flow.
Content cluster nodes are <em>distributor</em>, <em>content node</em> and <em>cluster controller</em>:
</p>
<img src="img/elastic-feed.svg" height="340" width="310"/>



<h2 id="node-state">Node state</h2>
<p>
Content and distributor nodes have state:
</p>
<table class="table">
<tr>
  <th>Up</th>
  <td>The node is up and available to keep buckets and serve requests.</td>
</tr><tr>
  <th>Down</th>
  <td>The node is not available, and can not be used.</td>
</tr><tr>
  <th>Initializing</th>
  <td>The node is starting up.
      It knows what buckets it stores data for, and may serve requests,
      but it does not know the metadata for all its buckets yet,
      such as checksums and document counts.
      The node is available for bucket placement.
  </td>
</tr><tr>
  <th>Stopping</th>
  <td>This node is stopping and is expected to be down soon.
      This state is typically only exposed to the cluster controller
      to tell why the node stopped.
      The cluster controller will expose the node as down
      or in maintenance mode for the rest of the cluster.
      This state is thus not seen by the distribution algorithm.
  </td>
</tr><tr>
  <th>Maintenance</th>
  <td>This node is temporarily unavailable.
      The node is available for bucket placement, redundancy is hence lower. <!-- ToDo rewrite -->
      Using this mode, new copies of the documents stored on this node will not be created,
      allowing the node to be down with less of a performance impact on the rest of the cluster.
      This mode is typically used to mask a down state during controlled node restarts,
      or by an administrator that need to do some short maintenance work,
      like upgrading software or restart the node.
  </td>
</tr><tr>
  <th>Retired</th>
  <td>A retired node is available and serves requests.
      This state is used to remove nodes while keeping redundancy.
      Buckets are moved to other nodes (with low priority), until empty.
      Special considerations apply when using
      <a href="../elastic-vespa.html#grouped-distribution">grouped distribution</a>
      as buckets are not necessarily removed.
  </td>
</tr>
</table>
<p>
Distributor nodes start / transfer buckets quickly
and are hence not in <em>maintenance</em> or <em>retired</em>.
<!-- ToDo: it should not be possible to set a distributor in this mode, check -->
</p>



<h2 id="cluster-state">Cluster state</h2>
<p>
There are three kinds of state:
</p>
<table class="table">
<tr id="unit-state">
  <th>Unit state</th>
  <td>
  <p>
  The cluster controller fetches states from all nodes, called <em>unit states</em>.
  States reported from the nodes are either <em>initializing</em>, <em>up</em> or <em>stopping</em>.
  If the node can not be reached, a <em>down</em> state is assumed.
  </p><p>
  This means, the cluster controller detects failed nodes.
  The subsequent <em>generated states</em> will hence have nodes in <em>down</em>,
  and the <a href="idealstate.html">ideal state algorithm</a> will redistribute
  <a href="buckets.html">buckets</a> of documents.
  </p>
  </td>
</tr><tr id="user-state">
  <th>User state</th>
  <td>
  <p>
  <em>user state</em> must be one of
  <em>up</em>, <em>down</em>, <em>maintenance</em> or <em>retired</em>:
  <ul>
  <li>Retire a node from a cluster -
      use <em>retired</em> to move buckets to other nodes</li>
  <li>Short-lived maintenance work -
      use <em>maintenance</em> to avoid merging buckets to other nodes</li>
  <li>Fail a bad node. The cluster controller or an operator can set a node <em>down</em></li>
  </ul>
  Use tools for <a href="../operations/admin-procedures.html#cluster-state">user state management</a>.
  </p>
  </td>
</tr><tr id="generated-state">
  <th style="white-space: nowrap">Generated state</th>
  <td>
  <p>
  The cluster controller <em>generates</em> the cluster state
  from the <em>unit</em> and <em>user</em> states, over time.
  The generated state is called the <em>cluster state</em>.
  </p>
  </td>
</tr>
</table>
<p>
For new cluster states, the cluster state version is upped,
and the new cluster state is broadcasted to all nodes.
There is a minimum time between each cluster state change.
<!-- ToDo ref to config here -->
</p><p>
It is possible to set a minimum capacity for the cluster state to be <em>up</em>.
<!-- ToDo ref to config here -->
If a cluster has so many nodes unavailable that it is considered down,
the state of each nodes is irrelevant,
and thus new cluster states will not be created and broadcasted
before enough nodes are back for the cluster to come back up.
A cluster state indicating the entire cluster is down,
may thus have outdated data on the node level.
</p>
<!-- ToDo: An example cluster state string is useful here -->



<h2 id="cluster-controller">Cluster controller</h2>
<p>
The main task of the cluster controller is to maintain the <a href="#cluster-state">cluster state</a>.
This is done by <em>polling</em> nodes for state,
<em>generating</em> a cluster state,
which is then <em>broadcast</em> to all the content nodes in the cluster.
Note that clients do not interface with the cluster controller -
they get the cluster state from the distributors - <a href="#distributor">details</a>.
<table class="table">
<tr id="node-state-polling">
  <th>Node state polling</th>
  <td>
  <p>
  The cluster controller polls nodes, sending the current cluster state.
  If the cluster state is no longer correct, the node returns correct information immediately.
  If the state is correct, the request lingers on the node,
  such that the node can reply to it immediately if its state changes.
  After a while, the cluster controller will send a new state request to the node,
  even with one pending.
  This triggers a reply to the lingering request and makes the new one linger instead.
  Hence, nodes have a pending state request.
  </p><p>
  During a controlled node shutdown, it starts the shutdown process
  by responding to the pending state request that it is now stopping.
  <strong>Note: </strong> As controlled restarts or shutdowns are implemented as TERM signals
  from the <a href="../config-sentinel.html">config-sentinel</a>,
  the cluster controller is not able to differ between controlled and other shutdowns.
  </p>
  </td>
</tr><tr id="cluster-state-generation">
  <th>Cluster state generation</th>
  <td>
  <p>
  The cluster controller translates unit and user states
  into the generated <em>cluster state</em>
  </p>
  </td>
</tr><tr id="state-broadcast">
  <th>Cluster state broadcast</th>
  <td>
  <p>
  When node unit states are received, a cluster controller internal cluster state is updated.
  New cluster states are distributed with a minimum interval between. <!-- ToDo link to config here -->
  A grace period per unit state too -
  e.g, distributors and content nodes that are on the same node often stop at the same time.
  </p><p>
  The version number is upped, and the new cluster state is broadcast.
  </p>
  </td>
</tr>
</table>
</p>


<h3 id="master-election">Master election</h3>
<p>
Vespa can be configured with one cluster controller.
Reads and writes will work well in case of cluster controller down,
but other changes to the cluster (like a content node down) will not be handled.
It is hence possible to configure a set of cluster controllers.
</p><p>
A set of cluster controllers elect a master among themselves.
The master does the node polling and cluster state broadcast.
The other cluster controller instances exist to participate in master election
and potentially take over if the master dies.
</p><p>
All cluster controllers will vote for cluster controller with the lowest index that says it is ready.
If a cluster controller has more than half of the votes, it will be elected master.
A fresh master will not broadcast states before a transition time is passed, <!-- ToDo config ref -->
allowing an old master to have some time to realize it is no longer the master.
</p>


<h3 id="historic-state">Historic state</h3>
<p>
Very little state is persisted by the cluster controllers.
If the master changes, or is restarted, historic knowledge built up is typically reset.
A few exceptions exist, being stored in a ZooKeeper cluster:
<ul>
  <li>The last cluster state version is stored, such that a cluster controller
      taking over can continue the version numbers where the old one left of.</li>
  <li>The user states set externally are stored, such that a reset does not
      remove an administrator to set a node down for instance.</li>
</ul>
Note: ZooKeeper data tends to be corrupted if the disk they store data on gets full.
It is thus recommended to not store ZooKeeper data on partitions
that are also used for logs or data storage, which are prone to getting filled.
If ZooKeeper data is corrupted, it must be deleted,
in which case all persisted state is cleared.
If having multiple nodes, the state may survive from a node where the data is not corrupt.
Otherwise the data is lost. User states must be reset.
Cluster state versions will start back at zero.
</p>


<h3 id="version-reset">Cluster State Version Reset</h3>
<p>
If ZooKeeper data goes missing, the cluster state version will be reset to zero.
This is not an issue for distributors and content nodes,
as these will trust that the most recently received cluster state is always the most correct one.
</p><p>
Clients use the versions to detect that a cluster state is upgraded.
The cluster controller tries to broadcast the cluster state at the same time to all the nodes,
but the distributors may process the new cluster state at a bit different times,
such that clients may alternatively talk to distributors having the newest cluster state or not.
By using the version numbers, the clients does not dump the newest cluster state
for one received by a distributor that have not processed the new one yet.
</p><p>
To handle a cluster state version reset,
clients have some functionality for detecting whether distributors seems to agree
the cluster state it has is wrong, even if version is newer.
In such cases clients will dump its cluster state and use a new one even if version is lower.
Clients thus handle a version reset,
but it may result in a few temporarily failures while it is confused.
</p>



<h2 id="distributor">Distributor</h2>
<p>
The distributor keeps track of which search nodes should hold which copy of each bucket,
based on the <em>redundancy</em> setting and information from the cluster controller.
Distributors are responsible for keeping track of metadata for a non-overlapping subset of the data in the cluster.
The distributors each keep a bucket database containing metadata of buckets it is responsible for.
This metadata indicates what content nodes store copies of the buckets,
the checksum of the bucket content and the number of documents and meta entries within the bucket.
Each incoming document is assigned to a bucket and forwarded to the right search nodes.
All requests related to these buckets are sent through the distributors.
</p><p>
A <a href="../elastic-vespa.html#bucket-distribution-algorithm">distribution algorithm</a>
calculates what distributors are responsible for given buckets based on
information of what nodes are available in the cluster.
Clients calculate what distributor to talk to using information in the cluster state.
</p><p>
When a distributor goes down or becomes available,
all distributors reshuffle what buckets they are responsible for.
During this reshuffling, clients may get some requests temporarily failing
as distributors taking over bucket ownership doesn't yet know where copies of its buckets are located.
These requests will automatically be resent, providing timeouts allow for it.
Changes in what distributors are up and available will currently cause windows of availability loss.
Other distributors take over bucket ownership.
To do this, they fetch bucket metadata for new buckets from all storage nodes,
in-memory operations for speed.
</p><p>
The distribution algorithm also calculates what content nodes copies of buckets should be stored on.
As moving bucket copies take time, the distributors track current state in a bucket database,
allowing requests to be processed independent of where buckets are currently located.
Distributors use the distribution algorithm to detect buckets that are stored on wrong nodes,
and move copies to correct nodes when there are free resources to do so.
</p><p>
In addition the distributor can track some historic knowledge.
For instance it may know that of the currently existing bucket copies,
a given copy has been available all the time and may be trusted to have all the information,
while another copy may be on a storage node that recently restarted,
so that copy may lack some documents.
Such historic state is not persisted, and is thus lost on distributor restarts.
</p><p>
The distributors use the bucket metadata to ensure the buckets are in a good state.
<ul>
  <li>If buckets have too few copies, new copies are generated</li>
  <li>If buckets have too many copies, superfluous copies are deleted once the
    distributor knows the copies to delete don't contain data other copies do not have</li>
  <li>If all the copies do not contain the same data,
    merges are issued to get bucket copies consistent</li>
  <li>If two buckets exist, such that both may contain the same document,
    the buckets are split or joined to remove such overlapping buckets</li>
  <li>If buckets contain too little or too much data, they should be joined or split</li>
  <li>If not exactly one copy is marked active, and the backend wants a copy to
    be marked active, activate or deactivate copies to get in a good state</li>
</ul>
The maintenance operations have different priorities.
The distributor prioritizes to fix the most critical issues first.
If no maintenance operations are needed, the cluster is said to be in the <em>ideal state</em>.
The distributors synchronize maintenance load with user load,
e.g. to remap requests to other buckets after bucket splitting and joining.
</p><p>
<!-- ToDo rewrite this -->
Clients use the cluster state to route to the correct distributor
using the <a href="idealstate.html">ideal state algorithm</a>.
They get updated cluster states through the distributors they talk to.
If clients use the wrong distributor for a bucket,
it will get the correct cluster state returned in the reply.
The client then calculates the correct distributor and resends the request.
That way, clients do not depend directly on the cluster controller
as distributors serve cluster states to clients.
</p>
