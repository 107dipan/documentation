---
# Copyright 2017 Yahoo Holdings. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Content nodes"
---


<h2 id="cluster-controller">Cluster controller</h2>
<p>
The <em>cluster controller</em> will:
<table class="table">
<thead></thead><tbody>
<tr>
  <th>Give the cluster a uniform view of its state</th><td>
    The main cluster controller task is to give the cluster a uniform view
    of what nodes are available and what they capacities are.
    To do this, it polls all distributor and content nodes for health and state information.
    All the states are combined into a cluster state which is again broadcasted.
  </td>
</tr><tr>
  <th>Allow administrators to override node states</th><td>
    If one wants a node to be less available than its health indicates,
    administrators can override node states in the cluster controller.
    Maintenance and retired modes are options to stop using nodes
    other than just stopping the services.
    Enforcing a node to be down also means that
    if someone accidentally starts the services again,
    it will not enter the live cluster.
  </td>
</tr><tr>
  <th>Automatically detect and fix cluster issues</th><td>
    The cluster controller may notice that a node is failing,
    such as constantly crashing.
    The cluster controller can detect this and stop using problematic nodes.
  </td>
</tr><tr>
  <th>Improving alerting and monitoring</th><td>
    As the cluster controller monitors content nodes' health,
    and keeps some node metrics,
    it is a good location to implement utilities
    to improve automatic detection of issues and cluster monitoring.
  </td>
</tbody>
</table>


<h3 id="uniform-state-view">Uniform state view</h3>
<p>
The main task of the cluster controller is to maintain the
<a href="admin-states.html">cluster state</a>.
This is achieved by <em>polling</em> nodes for state,
<em>generating</em> a cluster state,
which is then <em>broadcast</em> to all the content nodes in the cluster.
</p><p>
Clients use the cluster state to route to the correct distributor
using the <a href="idealstate.html">ideal state algorithm</a>.
They get updated cluster states through the distributors they talk to.
If clients use the wrong distributor for a bucket,
it will get the correct cluster state returned in the reply.
The client then calculates the correct distributor and resends the request.
That way, clients do not depend directly on the cluster controller
as distributors serves cluster states to clients.
</p><p>
With an updated cluster state,
distributors can detect that they are the correct target for the request,
and content nodes can verify that the request comes from the correct distributor node.
</p>
<img src="../img/design/clustercontroller-overview.png"/>


<h3 id="node-state-polling">Node state polling</h3>
<p>
The cluster state uses availability and capacity information
from all the distributor and content nodes.
The cluster controller retrieves this information
by polling all the nodes for health and state information.
</p><p>
To give the cluster controller an updated view, the cluster controller sends
requests with the currently known node state.
If the supplied state is no longer correct, the node returns correct information immediately.
If the state is correct, the request lingers on the node for a while, such that the
node can reply to it immediately if its state changes.
After a while, the cluster controller will send a new state request to the node,
even if it has one pending.
This will trigger a reply to the lingering request and make the new one linger instead.
This way, each node have a pending state request at almost all times,
such that new information can be quickly given to the cluster controller.
</p><p>
During a controlled node shutdown, it may thus start the shutdown process
by responding to the pending state request that it is now stopping.
<strong>Note: </strong> As controlled restarts or shutdowns are implemented as TERM signals
from the <a href="../config-sentinel.html">config-sentinel</a>,
the cluster controller is not able to differ between controlled and other shutdowns.
</p>


<h3 id="cluster-state-generation">Cluster state generation</h3>
<p>
The node states kept in the cluster state doesn't need to match the states
reported from the node themselves.
For instance, while a node may return that it is stopping to the cluster controller
to indicate a reason for it shutting down,
the cluster state will say that this node is down or in maintenance mode.
</p><p>
The cluster controller is also free to keep the node temporarily out of service.
For instance, if the node died while initializing last time it started,
the cluster controller may keep the node in the down state
until it verifies that the node managed to finish initializing this time around.
</p><p>
On top of this, there may be permanent state overrides for the node.
If the cluster controller detects the node constantly restarting,
creating unnecessary state flux that interrupts the cluster,
it may permanently set the node out of service until an administrator can fix it.
Administrators may also manually override the state to set the node temporarily out of service
while doing maintenance work, or removing it permanently due to some issues being manually detected.
</p><p>
The node states in the cluster states are thus calculated based on the
reported states from the node, and historic knowledge kept in the cluster controller.
If the result ends up in a state with higher availability than the user state for the node,
the state is adjusted based on it.
</p>


<h3 id="state-broadcast">Cluster state broadcasting</h3>
<p>
When node states are received, a cluster controller internal cluster state is updated.
To limit the amount of cluster states broadcasted to the cluster,
the cluster controller is not allowed to create a new official state before a
given time period has past since the last one.
Additionally it will wait for a short period of time after receiving a change,
in case there are other changes happening at about the same time that should be included too.
For instance, distributors and content nodes that are on the same node
often stop at the same time.
</p><p>
If the internal cluster state have changed enough from the last official
cluster state for the cluster to care,
enough time has passed since the last official state was generated (typically a few seconds),
and no more node state changes have been seen recently (typically less than a second),
a new official cluster state is generated based on the internal state.
The version number is upped, and the new official cluster state is immediately broadcasted to all the nodes.
</p>


<h3 id="master-election">Master election</h3>
<p>
A set of cluster controllers elect a master among themselves.
The master gathers <a href="admin-states.html">node states</a>
from all the distributor and content nodes, and combine them into a cluster state.
The cluster state is broadcasted to all the nodes every time it changes,
allowing all nodes to know what distributors are responsible for handling what parts of the data.
Clients picks up an updated cluster state if they try talking to the wrong distributor.
</p><p>
Having only a single cluster controller, if the controller goes down for any reason,
there is no longer a cluster controller alive to adjust the cluster state.
While this is not a single point of failure,
as typically a distributor or content node have to go down afterwards for it to matter much,
it would still be good if one could have more than one cluster controller
such that it was less critical for one to go down.
</p><p>
However, having two cluster controllers working at the same time,
the cluster could end up confused with the cluster controllers sending contradicting cluster states.
One of them may have failed to talk to node 3 while the other one may have failed to talk to node 5,
and then they send out two different cluster states.
The nodes in the cluster may receive the cluster states in different order,
and may end up confused with the state of the cluster.
</p><p>
To allow multiple controllers, but avoid confusion,
only a single cluster controller is allowed to broadcast cluster states.
This cluster controller is known as the master cluster controller.
The other cluster controller instances only exist to participate in master election
and potentially take over if the master dies.
</p><p>
All cluster controllers will vote for cluster controller with the lowest index that says it is ready.
If a cluster controller have more than half of the configured cluster controllers voting for it,
it will be elected master.
A newly elected master will not start broadcasting states before a transition time is passed,
allowing an old master to have some time to realize it is no longer the master.
</p><p>
This means, that for there to be a master with <code>N</code> nodes down,
<code>2 * N + 1</code> cluster controllers must be configured.
It also means only the cluster controllers with the <code>N</code> lowest indexes
will ever be able to become master.
The remaining cluster controllers will never do anything
but participate in the master election voting process.
</p>


<h3 id="historic-state">Historic state</h3>
<p>
Very little state is persisted by the cluster controllers.
If the master changes, or is restarted, historic knowledge built up is typically reset.
A few exceptions exist, being stored in a ZooKeeper cluster:
<ul>
  <li>The last cluster state version is stored, such that a cluster controller
      taking over can continue the version numbers where the old one left of.</li>
  <li>The user states set externally are stored, such that a reset does not
      remove an administrator to set a node down for instance.</li>
</ul>
Note: ZooKeeper data tends to be corrupted if the disk they store data on gets full.
It is thus recommended to not store ZooKeeper data on partitions
that are also used for logs or data storage, which are prone to getting filled.
If ZooKeeper data is corrupted, it must be deleted,
in which case all persisted state is cleared.
If having multiple nodes, the state may survive from a node where the data is not corrupt.
Otherwise the data is lost. User states must be reset.
Cluster state versions will start back at zero.
</p>


<h3 id="version-reset">Cluster State Version Reset</h3>
<p>
If ZooKeeper data goes missing, the cluster state version will be reset to zero.
This is not an issue for distributors and content nodes,
as these will trust that the most recently received cluster state is always the most correct one.
</p><p>
Clients use the versions to detect that a cluster state is upgraded.
The cluster controller tries to broadcast the cluster state at the same time to all the nodes,
but the distributors may process the new cluster state at a bit different times,
such that clients may alternatively talk to distributors having the newest cluster state or not.
By using the version numbers, the clients does not dump the newest cluster state
for one received by a distributor that have not processed the new one yet.
</p><p>
To handle a cluster state version reset,
clients have some functionality for detecting whether distributors seems to agree
the cluster state it has is wrong, even if version is newer.
In such cases clients will dump its cluster state and use a new one even if version is lower.
Clients thus handle a version reset,
but it may result in a few temporarily failures while it is confused.
</p>


<h3 id="monitoring">Monitoring cluster controller state</h3>
<p>
Basic cluster and node state information is available through the
<a href="api-state-rest-api.html">State Rest API</a>.
This API can also be used to set a user state for a node, e.g. forcing a node permanently down.
</p><p>
A few command line tools have been implemented using the State Rest API:
<ul>
  <li><a href="../reference/vespa-cmdline-tools.html#vespa-get-cluster-state">vespa-get-cluster-state</a></li>
  <li><a href="../reference/vespa-cmdline-tools.html#vespa-get-node-state">vespa-get-node-state</a></li>
  <li><a href="../reference/vespa-cmdline-tools.html#vespa-set-node-state">vespa-set-node-state</a></li>
</ul>
</p><p>
A status page for the cluster controller is available at the status port at
<code>http://hostname:port/clustercontroller-status/v1/<em>&lt;clustername&gt;</em></code>.
If <em>clustername</em> is not specified, the available clusters will be listed.
The cluster controller leader status page will show if any nodes are operating with differing cluster state versions.
It will also show how many data buckets are pending merging (document set reconciliation)
due to either missing or being out of sync.
<pre>
$ <a href="../reference/vespa-cmdline-tools.html#vespa-model-inspect">vespa-model-inspect</a> service container-clustercontroller | grep HTTP
</pre>
With multiple cluster controllers, look at the one with a "/0" suffix in its config ID;
it is the preferred leader.
</p><p>
The cluster state version is listed under the <em>SSV</em> table column.
Divergence here usually points to host or networking issues.
</p>


<h3 id="configuration">Configuration</h3>
<p>
To configure the cluster controller, use
<a href="/documentation/reference/services-content.html#cluster-controller">services.xml</a>
and/or add
<a href="https://github.com/vespa-engine/vespa/blob/master/configdefinitions/src/vespa/fleetcontroller.def">
configuration</a> under the <em>services</em> element - example:
<pre>
&lt;services version="1.0"&gt;
  &lt;config name="vespa.config.content.fleetcontroller"&gt;
    &lt;min_time_between_new_systemstates&gt;5000&lt;/min_time_between_new_systemstates&gt;
  &lt;/config&gt;
</pre>
</p>



<h2 id="distributor">Distributor</h2>
<p>
The distributor keeps track of which search nodes should hold which copy of each bucket,
based on the <em>redundancy</em> setting and information from the cluster controller.
Distributors are responsible for keeping track of metadata for a non-overlapping subset of the data in the cluster.
The distributors each keep a bucket database containing metadata of buckets it is responsible for.
This metadata indicates what content nodes store copies of the buckets,
the checksum of the bucket content and the number of documents and meta entries within the bucket.
Each incoming document is assigned to a bucket and forwarded to the right search nodes.
All requests related to these buckets are sent through the distributors.
</p><p>
A <a href="../elastic-vespa.html#bucket-distribution-algorithm">distribution algorithm</a>
calculates what distributors are responsible for given buckets based on
information of what nodes are available in the cluster.
Clients calculate what distributor to talk to using information in the cluster state.
</p><p>
When a distributor goes down or becomes available,
all distributors reshuffle what buckets they are responsible for.
During this reshuffling, clients may get some requests temporarily failing
as distributors taking over bucket ownership doesn't yet know where copies of its buckets are located.
These requests will automatically be resent, providing timeouts allow for it.
Changes in what distributors are up and available will currently cause windows of availability loss.
Other distributors take over bucket ownership.
To do this, they fetch bucket metadata for new buckets from all storage nodes,
in-memory operations for speed.
</p><p>
The distribution algorithm also calculates what content nodes copies of buckets should be stored on.
As moving bucket copies take time, the distributors track current state in a bucket database,
allowing requests to be processed independent of where buckets are currently located.
Distributors use the distribution algorithm to detect buckets that are stored on wrong nodes,
and move copies to correct nodes when there are free resources to do so.
</p><p>
In addition the distributor can track some historic knowledge.
For instance it may know that of the currently existing bucket copies,
a given copy has been available all the time and may be trusted to have all the information,
while another copy may be on a storage node that recently restarted,
so that copy may lack some documents.
Such historic state is not persisted, and is thus lost on distributor restarts.
</p><p>
The distributors use the bucket metadata to ensure the buckets are in a good state.
<ul>
  <li>If buckets have too few copies, new copies are generated</li>
  <li>If buckets have too many copies, superfluous copies are deleted once the
    distributor knows the copies to delete don't contain data other copies do not have</li>
  <li>If all the copies do not contain the same data,
    merges are issued to get bucket copies consistent</li>
  <li>If two buckets exist, such that both may contain the same document,
    the buckets are split or joined to remove such overlapping buckets</li>
  <li>If buckets contain too little or too much data, they should be joined or split</li>
  <li>If not exactly one copy is marked active, and the backend wants a copy to
    be marked active, activate or deactivate copies to get in a good state</li>
</ul>
The maintenance operations have different priorities.
The distributor prioritizes to fix the most critical issues first.
If no maintenance operations are needed, the cluster is said to be in the <em>ideal state</em>.
The distributors synchronize maintenance load with user load,
e.g. to remap requests to other buckets after bucket splitting and joining.
</p>
